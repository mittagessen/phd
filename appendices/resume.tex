\chapter{Résumé Long}

\begin{french}

\section{Introduction}

Cette thèse consiste en un certain nombre de publications qui étudient les
difficultés de la rétrodigitalisation de l'écriture arabe historique et propose
plusieurs méthodes pour faire progresser l'état de l'art en la matière. Bien
que ces méthodes visent principalement à remédier les problèmes résultant des
caractéristiques de l'écriture arabe, elles sont conçues pour être applicables
à l'écriture et aux inscriptions dans une variété d'autres systèmes d'écriture.

\section{L'analyse d'imges de documents et reconnaissance optique de caractères}

L'analyse d'images de documents (AID) est un sous-domaine de la vision par
ordinateur (VO) qui cherche à comprendre le contenu des documents par
le traitement de leurs images numériques.  Les documents sont définis
de façon libérale par le domaine, comprenant pas seulement les
documents manuscrits et le texte imprimé sur papier, mais aussi
l'écriture sur d'autres supports souples tels que le papyrus ou les
feuilles de palmier et même les inscriptions.

La différence vis-à-vis de la vision par ordinateur en général ne se trouve pas
dans les méthodes employées, mais dans la nature des images traitées.
Ces images sont généralement obtenues par des caméras ou des scanners,
souvent dans un contexte professionnel, ce qui permet d'obtenir un
matériel source avec un minimum de bruit provenant d'éléments non
pertinents que l'on rencontre souvent dans les images de scènes
naturelles traitées par d'autres branches du VO. Malgré des données d'entrée
plus propres, les représentations structurées souhaitées en sortie ont
tendance à être plus complexes et plus nombreuses dans la AID que dans
d'autres applications, nécessitant la détection, la classification et
la mise en relation de dizaines d'éléments de documents tels que des
lignes, des caractères, des illustrations et des tableaux.

Comme dans d'autres domaines de l'informatique, la recherche en DIA peut être
divisée en tâches spécifiques, dont une ou plusieurs sont résolues par
une méthode particulière proposée. La tâche la plus importante de la
DIA est la reconnaissance optique de caractères (ROC), mais d'autres,
basées sur la ROC ou entièrement nouvelles, telles que la
reconnaissance de documents de classification, de datation ou de
repérage par mot clé, existent. La ROC, la conversion de textes imprimés,
écrits ou inscrits en texte codé par machine, est un processus établi depuis
longtemps, à la fois en tant que tâche dans la recherche sur la vision
par ordinateur et son utilisation pratique dans des applications comme
l'analyse des adresses ou les aides aux aveugles. Il est sans doute à
l'origine de toute AID avec un certain nombre de brevets remontant
au début du XIXe siècle.

Ces premières approches, des décennies avant les premiers ordinateurs, ont
maintenant évolué en l'application quotidienne des techniques de DIA
pour des tâches telles que l'analyse des adresses pour la distribution
du courrier, la vérification des chèques et la rétro-numérisation des
livres. En effet, on revendique aujourd'hui dans la profession que la
ROC est fondamentalement résolue, au moins pour les documents imprimés
modernes en anglais avec un niveau de bruit raisonnablement faible, où
les logiciels commerciaux modernes de rétrodigitalisation atteignent
régulièrement des taux de précision des caractères supérieurs à 99\%.
Néanmoins, il existe près de 4000 langues écrites et plusieurs centaines de
systèmes d'écriture, pour la grande majorité desquels des systèmes de
ROC pratiques ne sont pas disponibles. Même en considérant seulement
l'utilisation d'écritures purement alphabétiques telles que le latin et
le cyrillique, qui posent moins de problèmes à la ROC de pointe
lorsqu'elles sont employées conformément aux pratiques typographiques
occidentales modernes, il est clair qu'une part importante de la
production littéraire humaine n'est pas encore accessible par la
rétro-numérisation.

C'est encore plus clair pour le matériel historique. Les projets de
numérisation à grande échelle dans les pays riches ont permis de créer de
vastes collections numériques de les bibliothèques qui sont de facto
inaccessibles au public et aux chercheurs, même pour des documents aussi
récents que ceux de la fin du XIXe siècle car les variations orthographiques et
typographiques dégradent considérablement la qualité du texte numérisé par les
logiciels adaptés aux documents modernes. Il s'agit très probablement d'un état
de fait temporaire pour les documents les plus nombreux dans les archives des
pays développés où des projets tels que OCR-D\footnote{\url{http://ocr-d.de}}
ouvrent la voie pour transférer les progrès de la recherche pure en AID à la
pratique des bibliothèques. Dans le cadre de ces projets et d'efforts plus
spécifiques tels que \cite{smith2018research}, on a vu la cristallisation d'un
programme de recherche collective pour la numérisation du matériel historique
et des systèmes d'écriture minoritaires, partagé par les chercheurs en sciences
humaines engagés dans les méthodes numériques et les experts de la vision par
ordinateur. Néanmoins, ces communautés restent fracturées sur le plan
géographique, les frontières linguistiques et professionnelles.

D'autre part, le manque de financement combiné à la détérioration des
collections dans les autres pays en raison de conflits et d'influences
environnementales augmente le risque de perte permanente du patrimoine culturel
qui n'intéresse qu'un petit nombre de chercheurs et de populations
minoritaires. Même des collections célèbres telles que les manuscrits de
Tombouctou ont à peine échappé à la destruction par les conflits de ces
dernières années et sont en grand danger d'être détruites par l'humidité. 

\subsection{Tâches}

En tant qu'application centrale de l'analyse d'images de documents, la ROC
s'est divisée en un grand nombre de sous-problèmes. Ils ne sont pas tous
strictement nécessaires à un système ROC fonctionnel et, en fait, beaucoup
d'entre eux ne peuvent être mis en œuvre que de manière spécifique aux
matériaux et sont donc relégués à des applications spécialisées.

Le pipeline typique de la reconnaissance optique de caractères est construit
autour de quatre étapes de traitement de base :

\begin{description}
\item [Prétraitement] Le débruitage, le redressement, et la binarisation
\item [Segmentation des pages] Extraction des informations structurelles des
images de pages de documents et les enrichir avec des informations sémantiques
supplémentaires.
\item [Transcription] Extraction d'informations textuelles de la totalité ou
d'un sous-ensemble des objets identifiés à l'étape précédente.
\end{description}

Si cette caractérisation est valable pour tous les pipelines de données, sauf
les plus ésotériques, les blocs fonctionnels exacts dépendent fortement du type
et de la structure des documents à traiter. Chacune de ces étapes de traitement
contient une ou plusieurs méthodes qui servent à résoudre une tâche
particulière, comme par exemple:

\begin{description}
\item [Binarisation] La classification des pixels d'une image en deux classes :
le front, c'est-à-dire le texte, et le fond, c'est-à-dire tout le
reste.
\item [Débruitage] Augmenter la qualité de l'image de la page pour les tâches
	suivantes. Le débruitage comprend des processus tels que la
		normalisation de fond ou l'élimination des taches.
\item [Deskewing and Dewarping] Corriger à la fois la distorsion de perspective
	inhérente à la capture par caméra et d'autres dégradations introduites
couramment pendant de scanning telles que la
rotation, le déformation le long de la reliure, \dots
\item [Segmentation en regions]  Subdiviser une image de page en éléments tels que du texte, de la décoration, des notes, des points, etc.
\item [Segmentation en lignes] Extraction des lignes de texte d'une image de page.
\item [Segmentation en caractères] Segmentation du texte sur une image de page
	jusqu'au niveau du glyphe ou même à un niveau inférieur. Bien qu'il
		s'agisse d'une opération courante dans les systèmes ROC
		traditionnels, elle est le plus souvent superflue avec les
		méthodes de pointe.
\item [Classification du système d'écriture et de la police de caractères] Classifier la langue, le système d'écriture, le style ou la police de
caractères du texte. Cette classification peut être effectuée à
différents niveaux, par exemple à l'échelle du document ou
individuellement pour tout ou partie d'une ligne.
\item [Reconnaissance des tableaux] Inférer la structure logique des images des tableaux.
\end{description}

\section{Motivation et contributions scientifiques}

L'écriture arabe représente l'une des plus grandes traditions littéraires de
l'histoire de l'humanité, tant en termes de volume que de diffusion
géographique. Les exemples vont des textes religieux, en particulier le Coran,
le livre saint de l'Islam, à la poésie, en passant par les textes scientifiques
et juridiques, sans oublier un vaste corpus de documents administratifs. Le
nombre considérable de ces textes dans une multitude de domaines en fait une
cible privilégiée pour les nouveaux paradigmes des sciences humaines qui
utilisent des méthodes computationnelles telles que la lecture à distance et la
paléographie quantitative. Ces méthodes nécessitent soit de grands corpus de
textes, soit des méthodes précises de AID basées sur une ou plusieurs des
tâches susmentionnées d'un système ROC. Comme la grande majorité des textes
arabes n'ont jamais existé sous forme numérique, la rétro-numérisation de haute
qualité par ROC constitue la base d'un nombre important de projets de recherche
en sciences humaines numériques arabes.

Lorsque j'ai commencé à travailler sur cette thèse, la ROC du texte arabe
imprimé a été largement rejetée par les chercheurs en sciences humaines
travaillant sur ces documents, même ceux qui étaient déjà très impliqués dans
les sciences humaines numériques, et par les organismes de financement comme
peu pratique, encore plus pour le matériel historique ou multigraphique. La
reconnaissance précise de l'écriture à la main arabe semblait totalement
inatteignable. Bien qu'il existe une longue tradition de publications sur la
ROC d'ensembles de données arabes simples tels que KHATT\cite{mahmoud2014khatt}
et qu'un certain nombre de logiciels de ROC ouverts et propriétaires tels que
Tesseract, Abbyy FineReader et Sakhr offrent un soutien nominal pour la
reconnaissance de textes en caractères arabes, ces solutions ne se sont jamais
concrétisées dans la pratique réelle des bibliothèques universitaires ou à
grande échelle. Les raisons en sont multiples : taux d'erreur élevé des
classificateurs et des segmenteurs mal adaptés à la nature cursive du système
d'écriture, manque de logiciels et de compétences techniques facilement
disponibles, et coûts et efforts substantiels nécessaires pour adapter les
solutions existantes au matériel d'intérêt.

Il est rapidement apparu que les obstacles qui empêchent les chercheurs
travaillant sur des textes imprimés et manuscrits en caractères arabes
d'utiliser la ROC dans la pratique sont les mêmes que ceux rencontrés par de
nombreux autres chercheurs engagés dans la rétro-numérisation de documents
historiques et non-latins. Imitant l'opinion dominante sur la ROC arabe,
\cite{widner2017toward} a revendiqué que les manuscrits médiévaux (en
caractères latins) étaient pratiquement imperméables à la ROC contemporaine. On
peut probablement trouver des évaluations similaires pour d'autres domaines.

En tant que telle, cette thèse doit être considérée à l'intersection plus
générale entre les sciences humaines et l'informatique. La recherche qui y est
présentée n'est pas un ensemble de méthodes non connectées permettant de
résoudre des tâches uniques dans l'AID arabe, mais fait partie d'un écosystème
cohérent pour l'AID des sciences humaines composé de deux éléments principaux :
le système ROC Kraken et l'environnement de recherche virtuel (ERV)
eScriptorium.

Le Kraken est un système ROC complet et modulaire. Il se distingue des autres
solutions de plusieurs façons importantes : les chercheurs en sciences humaines
à tendance numérique comme utilisateurs et la conception du logiciel qui
cherche une flexibilité maximale. Il a également été largement adapté en vue
d'un meilleur traitement des documents historiques en caractères non latins,
notamment en arabe.

Dans le cadre des travaux d'adaptation visant à faire de Kraken un outil plus
performant pour le travail sur les textes arabes, deux études de faisabilité de
la rétro-numérisation de livres imprimés en caractères arabes classiques et
d'une importante revue de langue arabe publiée par l'Université Américaine de
Beyrouth ont été réalisées, qui ont produit la première analyse détaillée sur
les faiblesses et les capacités des méthodes de ROC de pointe sur les textes
arabes imprimés.

Cette thèse contribue de deux systèmes de segmentation de lignes entraînables,
un système élémentaire capable de détecter uniquement les lignes de base, et un
système plus avancé permettant la segmentation des régions et des lignes en
plus de la classification. Ce dernier est inclus dans le Kraken et permet la
détection conjointe de régions et de lignes et l'inférence de l'orientation des
lignes. Cette seconde méthode a également été optimisée pour l'utilisation de
la mémoire, réduisant la consommation de mémoire d'environ cinquante pour cent
par rapport aux réseaux de neurones à segmentation sémantique à convolution
totale, ayant des performances similaires.

Un nouveau backend de réseau neuronal a été ajouté au Kraken. Basé sur la
bibliothèque de réseaux de neurones pytorch, il permet la reconfiguration
flexible des réseaux de neurones artificiels (ARN) utilisés à la fois pour la
segmentation des pages et la transcription du texte grâce à un langage de
définition d'ARN léger qui est capable d'exprimer de nombreuses
caractéristiques des architectures courantes utilisées dans les tâches de
vision par ordinateur. Cette couche permet l'ajout relativement simple de
nouveaux types d'ARN et donc un prototypage rapide et une optimisation efficace
des hyperparamètres, même pour les utilisateurs finaux sans connaissances
approfondies en matière d'apprentissage machine, comme l'a démontré
\cite{strobel2020much}.

Au cours des premières études de cas, plusieurs milliers de lignes de données
de formation pour la transcription de textes ont été annotées et mises à la
disposition du public. J'ai participé à la conceptualisation technique et aux
directives de transcription pour ces ensembles de données. Un autre ensemble de
données sous licence libre de quatre cents pages de manuscrits en écriture
arabe dans une variété de langues, de styles et de domaines a été annoté avec
des lignes de base et l'orientation des lignes pour permettre l'évaluation des
méthodes d'analyse de la mise en page sur des documents historiques en écriture
arabe.  Cet ensemble de données très complexe reste le seul ensemble de données
manuscrites non latines pour le paradigme de base de l'analyse de la mise en
page.

La deuxième composante de cet écosystème de rétro-numérisation est l'ERV
eScriptorium.  Alors que le Kraken est conçu pour une flexibilité maximale,
l'eScriptorium adopte une autre approche. Conçu comme un environnement de
recherche paléographique et de publication à part entière pour un usage
scientifique, la fonctionnalité ROC n'est qu'une petite partie des
caractéristiques prévues. Comme il n'est pas pratique d'exposer toutes les
fonctionnalités du Kraken sur une telle plate-forme, la conception
d'eScriptorium permet une intervention manuelle et semi-automatique à chaque
étape du processus, soit par une manipulation manuelle dans l'interface, soit
par des interfaces d'échange de données graphiques et programmatiques.

Comme eScriptorium vise à offrir des fonctions scientifiques supplémentaires
au-delà de la simple rétro-numérisation, la plateforme est également un cadre
de test idéal pour la recherche en sciences humaines assistée par la vision
artificielle. Dans certains cas, ces fonctions avancées sont liées à la
transcription de textes. Une méthode permettant de dériver les emplacements des
graphèmes à partir de l'alignement implicite produit par une reconnaissance de
texte par ligne ANN et une évaluation sur des fragments de matériel hébraïque
est présentée.

\section{L'écriture arabe}

Le système d'écriture arabe est l'un des systèmes d'écriture les plus répandus
de l'histoire de l'humanité, géographiquement et chronologiquement. Il s'agit
de l'écriture principale de la langue arabe, du farsi, de l'ourdou et de
plusieurs autres langues du sous-continent indien. Historiquement, elle a été
utilisée pour produire des textes de l'espagnol au chinois.

Bien que ses origines exactes soient encore controversées, les historiens
s'accordent à dire que l'écriture arabe a évolué à partir de l'écriture
nabatéenne ou syriaque au Moyen-Orient au cours de plusieurs siècles, la
maturation ayant eu lieu au cours du septième siècle de notre ère.  Étroitement
liés à la propagation de l'Islam, un certain nombre de variantes alphabétiques
et de styles calligraphiques régionaux se sont développés au cours des siècles
suivants. Néanmoins, on est loin d'une écriture purement liturgique avec une
abondance de documents administratifs, de traités philosophiques et
scientifiques, de poésie, etc.

Il est donc erroné de parler d'une seule écriture arabe du point de vue des
recherches de l'AID. Chaque style, en combinaison avec les préférences
régionales et le contexte de leur utilisation, présente des défis particuliers.

\subsection{Les principes de l'écriture arabe}

L'écriture arabe est un \emph{abjad}, un système d'écriture consonantique, ce qui
signifie qu'elle ne nécessite que l'écriture des consonnes et des voyelles
longues, le lecteur étant censé fournir lui-même les voyelles courtes
appropriées selon le contexte.  Les voyelles courtes et autres marques pour des
caractéristiques telles que le doublage (gémination) et la nunation (ajout d'un
n final), peuvent être ajoutées en option (\emph{tashkil}) mais ne sont
systématiquement utilisées que lors de la transcription du Coran ou de textes
élémentaires pour les apprenants en langues. Comme le syriaque et l'hébreu, il
s'écrit de droite à gauche, à l'exception des nombres qui s'écrivent de gauche
à droite.

Contrairement à d'autres écritures très courantes comme le latin, l'arabe est
écrit uniquement dans une forme cursive. Comme les variantes cursives d'autres
écritures, les lettres individuelles changent de forme en fonction de leur
position dans un mot : des formes initiales, médianes, finales et indépendantes
existent, en plus des règles calligraphiques spécifiques au style pour le
placement des lettres. Une différence avec l'écriture cursive des autres
écritures est que toutes les lettres ne peuvent pas être reliées entre elles.
Comme les espaces n'indiquent pas nécessairement le début d'un nouveau mot, les
calligraphes sont largement libres de faire varier l'espacement entre les mots,
les syllabes et les coups comme ils le souhaitent.  Cette variation de
l'espacement peut perturber les systèmes de reconnaissance optique des
caractères.

Une autre distinction est l'absence de majuscule et la ponctuation de type
occidental, cette dernière n'ayant été introduite qu'au XXe siècle de notre
ère. Au lieu de cela, des mots et des phrases particuliers sont utilisés pour
introduire une nouvelle phrase ou question et les accents sont placés au-dessus
des titres et des rubriques.

Il existe d'autres formes de lettres dans des variantes de l'écriture arabe
adaptées à d'autres langues qui comportent des phonèmes qui ne se trouvent pas
en arabe. Elles sont généralement adaptées en ajoutant des points aux graphèmes
représentant des sons similaires en arabe, comme le persan \emph{pe} dérivé de
\emph{bāʾ} en ajoutant deux points supplémentaires ci-dessous. Comme il y a
beaucoup de langues qui sont ou ont été écrites dans l'écriture, il existe un
grand nombre de ces variantes.

Une difficulté particulière pour les systèmes ROC est la façon particulière
dont le texte arabe est justifié. La césure, c'est-à-dire la division des mots
pour faciliter le retour à la ligne, est absente de tous les textes, sauf les
plus anciens. Au lieu de la justification par espace blanc, courante dans les
écritures alphabétiques, des alternatives plus agréables visuellement ont été
conçues.  L'allongement des liens entre les lettres, la courbure de la ligne de
base et la superposition ou le déplacement de fragments du dernier mot
au-dessus ou à côté de la ligne sont des phénomènes courants, ces deux
dernières méthodes posant un défi particulier, même aux systèmes modernes de
segmentation des pages.

\subsection{Styles}

Un grand nombre de styles calligraphiques ont été conçus au cours des siècles.
Si certains sont reconnus dans l'ensemble du monde islamique, comme le
\emph{naskh}, d'autres sont spécifiques à certaines zones géographiques, par
exemple le \emph{maghribi} nord-africain ou le \emph{nastaʼlīq} persan. Les
premiers styles tels que le \emph{ḥijāzī} et le coufique sont des familles de
styles différents, car la standardisation était faible. Au treizième siècle de
notre ère, six styles canoniques étaient apparus. Ils sont généralement
appariés, une écriture d'affichage (majuscule) et une écriture de texte
(minuscule) :

\begin{itemize}
        \item \emph{ṯuluṯ} et \emph{naskh}
        \item \emph{muḥaqqaq} et \emph{rayḥān}
        \item \emph{tawqīʿ} et \emph{riqāʿ}
\end{itemize}

Les styles régionaux n'étaient pas seulement liés aux zones géographiques mais
aussi à l'utilisation de la langue.  Dans les parties du monde islamique
influencées par le persan (Empire Ottoman, Iran, Inde), des styles suspendus
tels que \emph{nastaʼlīq} avec des mots individuels descendant sur une ligne de
base commune étaient populaires car ils étaient plus adaptés aux différentes
combinaisons de lettres du turc et du persan. D'autres styles étaient à la fois
géographiquement limités et restreints à certains usages comme le
\emph{divanî} style chancellerie ottomane.

\subsection{Critères pour les systèmes de ROC arabes}

En résumé, la reconnaissance des textes arabes imprimés et manuscrits nécessite
un certain nombre de caractéristiques que l'on ne trouve pas couramment dans
les systèmes ROC actuels. Les principales exigences sont, dans l'ordre de
traitement à l'intérieur d'un pipeline typique :

\begin{description}
	\item[Èlimination de la binarisation] La variété des supports, des encres et des décorations utilisés
		dans les textes arabes rend peu probable le développement d'une
		méthode générale de binarisation. Une système ROC arabe devrait
		donc être sans binarisation.
	\item[Segmentation des lignes courbes et inclinées] L'utilisation
		fréquente de lignes inclinées et courbes à des fins tant
		pratiques qu'esthétiques nécessite une méthode de segmentation
		des pages capable de les extraire et de les représenter
		efficacement.
	\item[Segmentation sémantique des pages] L'utilisation extensive du
		paratexte nécessite un système de segmentation capable de
		séparer plusieurs textes sur la même page de document.
	\item[Détermination de l'ordre de lecture avancée] En plus de
		classifier les différents éléments textuels d'une page, il est
		également nécessaire de les mettre dans le bon ordre pour une
		véritable compréhension du document.
	\item[Transcription sans segmentation] Comme l'arabe s'écrit uniquement
		sous forme cursive et que les liaisons entre les lettres
		peuvent changer de manière significative d'un style à l'autre,
		une méthode de transcription arabe devrait fonctionner sur des
		lignes entières au lieu de les séparer en caractères.
	\item[Les outils de création et de conservation des données]
		Même s'ils ne font pas directement partie d'un pipeline de ROC,
		les outils ergonomiques qui peuvent être utilisés pour annoter
		toute la gamme des caractéristiques du texte arabe sont
		essentiels pour les projets de numérisation concrets et pour
		créer des ensembles de données pour les recherches futures.
\end{description}

\section{Études sur le ROC en écriture arabe}

Deux études de précision sur un certain nombre de textes classiques en écriture
arabe et sur la principale revue savante de langue arabe, al-Abhath, ont été
réalisées avec une version antérieure du système ROC Kraken afin de déterminer
les points forts et les limites des logiciels ROC actuels sur ces documents. En
conséquence, nous avons déterminé deux recommandations clés pour améliorer
substantiellement la ROC en écriture arabe : (1) une approche plus systématique
de la production de données de l'entrainement, et (2) le développement de
composants technologiques clés, en particulier des modèles multilingues et une
meilleure méthode de segmentation des lignes et de la mise en page.

La première étude préliminaire a montré que malgré les faibles taux d'erreur de
caractères (<3\%) obtenus par les modèles spécifiques aux polices de
caractères, les différences substantielles entre les polices de caractères se
traduisent par des taux d'erreur nettement plus élevés desdits modèles sur des
polices de caractères dissemblables. Bien que cette mauvaise généralisation
puisse être attribuée dans une certaine mesure aux limites du réseau de
transcription (LSTM peu profond entraîné sans régularisation) dans cette
première version du Kraken, elle a montré la nécessité à la fois d'un backend
de réseau neuronal plus puissant dans le système et d'une sélection minutieuse
des données d'entraînement pour assurer la représentativité de l'ensemble des
données d'entraînement pour le domaine cible souhaité.

La seconde étude a été construite dès le début autour d'une analyse beaucoup
plus rigoureuse des polices de caractères présentes dans le matériel et d'une
évaluation manuelle approfondie des résultats de la ROC par rapport à une
évaluation purement computationnelle. Son sujet, la revue al-Abhath, a été
déterminée à comporter deux groupes de polices de caractères, bien qu'il y ait
de légères variations intra-groupe, en particulier pour la première police de
caractères qui a été utilisée pour la majorité du cycle de la revue. Pour se
conformer à cette analyse, il a été décidé de produire 5000 lignes de données
d'entraînement pour la première police de caractères et 2000 pour la seconde.
Les lignes d'entraînement ont été tirées au hasard des deux groupes de
caractères et transcrites manuellement avec le logiciel CorpusBuilder. Des
modèles de transcription spécifiques aux polices de caractères ont été
entraînés sur les deux ensembles de données, puis évalués par un prestataire
extérieur et OpenITI sur un ensemble de validation distinct (voir tableaux
~\ref{tab_champs:table_1} et ~\ref{tab_champs:table_2}). À l'issue de l'examen,
il est apparu clairement que Kraken surpasse significativement le logiciel ROC
d'Abbyy pour la reconnaissance de textes arabes. Une évaluation manuelle
détaillée de la transcription automatique a permis d'identifier plusieurs types
d'erreurs courantes : mauvaise reconnaissance de ligatures peu courantes, de
formes de lettres rares et de caractères d'allongement, de texte multigraphique
et le sortie de lettres doublées. Bien que certains de ces problèmes aient pu
être résolus grâce à des données d'entraînement de meilleure qualité, le
traitement de texte multigraphique dans le système ROC a été ajouté
subséquemment (chapitre~\ref{ch:multi}). L'évaluation a également identifié le
besoin d'une meilleure segmentation des lignes des textes arabes, car le module
optimisé pour le latin dans le Kraken avait tendance à tronquer et à diviser
les lignes de texte avec une certaine fréquence.

\section{Segmentation des pages}

Pour le traitement des documents imprimés et manuscrits en caractères arabes,
il est évident qu'une méthode d'analyse de la mise en page flexible, et de
préférence entraînable, est nécessaire, vu que l'impossibilité d'extraire des
lignes de texte rend automatiquement impossible leur transcription correcte
avec n'importe quelle méthode de reconnaissance de texte.

Alors qu'il existe un grand nombre d'ensembles de données d'entraînement
suivant différentes représentations de lignes de texte pour les documents
modernes et historiques en écriture latine, ce n'est pas le cas pour les
manuscrits arabes. Pour aider au développement, nous avons préparé un ensemble
de données de 400 pages de manuscrits arabes et persans annotés avec leurs
lignes de texte.

Il est important de comprendre la nature exacte de la segmentation des lignes
de texte dans un pipeline ROC et son lien avec la méthode de transcription. Si
l'objectif premier est d'identifier les lignes de texte, la tâche d'un
segmenteur de lignes de texte est également d'aider à l'extraction des lignes
de manière à optimiser les performances de la transcription. Ainsi, différentes
représentations de lignes de texte peuvent être produites par un système de
segmentation, par exemple des boîtes englobantes, des polygones englobants, des
nuages de pixels, des lignes de base, etc. Tous ces éléments ne sont pas
adaptés aux conventions calligraphiques de l'écriture arabe. Par exemple, le
tracé d'un rectangle englobant une ligne courbe ou inclinée inclura
nécessairement des parties de lignes adjacentes à l'intérieur du rectangle.  En
outre, la précision de la transcription est améliorée lorsque les lignes sont
centrées à l'intérieur de la bande d'image rectangulaire introduite dans le
réseau de transcription.

Une représentation capable à la fois d'encoder une ligne sans bruit adjacent et
de permettre la normalisation de la ligne sur une ligne droite est donc
hautement souhaitable pour les documents qui contiennent des lignes non droites
avec une certaine régularité.  Pour l'ensemble de données, la représentation de
la ligne de base a été choisie car elle est à la fois rapide à annoter,
relativement facile à apprendre par les méthodes de vision par ordinateur, et
suffisamment polyvalente pour permettre la normalisation de lignes de forme
arbitraire.

Une ligne de base n'est qu'une ligne virtuelle sur laquelle la plupart des
caractères reposent. Alors qu'ils soient généralement droits dans les documents
imprimés, ils peuvent être définis comme des polylignes, ce qui leur permet de
suivre la courbure de la ligne. En projetant les éléments courbes sur une ligne
de base droite, nous pouvons transformer une ligne courbe en une ligne droite
pouvant être traitée par le modèle de transcription. Si la ligne de base est
associée à un polygone englobant, il est également possible de supprimer des
éléments en dehors de la ligne de texte qui nous intéresse dans le processus.

Nous avons proposé une méthode de segmentation, basée sur un réseau neuronal de
segmentation sémantique convolutive profonde (U-Net), suivant cette
représentation de la ligne de base et l'avons évaluée sur notre ensemble de
données et un ensemble de données latins cBAD. La méthode a atteint une
précision similaire à celle d'autres méthodes de pointe (voir
tableau~\ref{tab:foo}).   Bien que les résultats obtenus sur l'ensemble de
données arabes aient été inférieurs à ceux obtenus sur l'écriture latine, notre
ensemble de données est beaucoup plus diversifié, ce qui indique la pertinence
générale de cette approche pour la segmentation des lignes de texte des
manuscrits arabes.

Bien qu'efficace, cette méthode manquait de plusieurs caractéristiques
nécessaires à un segmenteur pratique. Premièrement, elle ne comporte pas de
moyen de calculer l'orientation des lignes, deuxièmement, elle ne dispose pas
d'un algorithme pour calculer un polygone englobant pour la suppression du
contenu non-ligne, et elle est incapable de reconnaître conjointement les
lignes et les régions. En changeant la couche de sortie du réseau neuronal pour
effectuer une classification de pixels multi-étiquettes, la nouvelle méthode
est capable de détecter à la fois les lignes et les régions simultanément. Les
nouvelles classes de marqueurs dans la carte de sortie indiquent quelle
extrémite d'un ligne est le début et quelle fin permet de déterminer
l'orientation de la ligne. En outre, une nouvelle méthode de post-traitement a
été proposée pour extraire les lignes de base de la sortie de la carte de
pixels brute du réseau de segmentation sémantique et pour calculer un polygone
englobant. Enfin, le réseau de segmentation sémantique a été changé en un
réseau de type ReNet plus efficace en mémoire qui utilise des couches LSTM
balayées verticalement et horizontalement au lieu de piles profondes de couches
convolutionnelles pour obtenir de grands champs récepteurs.

Cette deuxième méthode a été évaluée à la fois sur l'ensemble de données
arabes, une nouvelle version de l'ensemble de données cBAD et un certain nombre
d'ensembles de données latines plus petites. Les résultats étaient comparables
à la pointe de la technologie pour la segmentation des régions et des lignes de
texte, avec une certaine amélioration par rapport à la méthode précédente dans
les résultats sur l'ensemble de données arabes.

Enfin, nous avons proposé une méthode simple pour la détection du système
d'écriture et de I'emphase dans le texte lignes. Ce système est utile pour le
traitement des textes et documents multilingues où l'emphase, c'est-à-dire le
texte en italique, en gras, etc. est utilisée pour le balisage sémantique, tel
qu'il se produit fréquemment dans les dictionnaires.

La méthode profite de l'alignement implicite fourni par le réseau de
transcription de texte formé avec la fonction de coût CTC.  Bien qu'il ne soit
pas garanti que les activations pour un caractère particulier soient proches de
son emplacement dans la ligne, les capacités limitées de modélisation à longue
distance d'un réseau LSTM font qu'il le place presque toujours correctement.
En entrainant un réseau de transcription de texte à produire une séquence de
codes d'identification au lieu de caractères réels, nous pouvons diviser une
ligne en bandes appartenant à un seul système d'écriture. Ces bandes peuvent
ensuite être traitées par des modèles de reconnaissance spécifiques à
l'écriture. Une propriété intéressante de notre approche est que le système
peut être entraîné et que les données de son apprentissage peuvent être
dérivées automatiquement des données d'apprentissage existantes pour les
modèles de transcription.

\section{La transcription et l'alignement}

\subsection{Le logiciel ROC Kraken}

Le Kraken est un logiciel de ROC modulaire et open source conçu pour être
particulièrement utile pour la rétro-numérisation dans les sciences humaines.
Outre les méthodes de pointe pour la transcription et l'analyse de la mise en
page, il comprend un certain nombre d'autres fonctionnalités qui le rendent
intéressant pour les chercheurs en humanités.

Un grand soin a été apporté à son développement pour réduire les hypothèses
implicites sur le fonctionnement du texte et pour rendre ses limitations
explicites. Il a été étendu depuis ses origines en tant que bifurcation du
système OCRopus avec un support Unicode complet de droite à gauche,
bidirectionnel et vertical de l'écriture, la détection des scripts et la
reconnaissance multigraphique. Une interface JSON simple permettant la
configuration d'un mappage entre les sorties de modèles numériques et les
séquences de points de code Unicode et vice versa. Ce mécanisme est
particulièrement utile pour les écritures logographiques de grande dimension
telles que le système d'écriture chinois car il permet la décomposition d'un
point de code Unicode représentant un seul groupe de graphèmes en ses
composants logiques dans la sortie du réseau neuronal.

Comme le Kraken est conçu pour être facilement intégré dans d'autres
applications, il offre à la fois une API simple et un système de sérialisation
flexible grâce à des templates. Des templates pour un certain nombre de formats
tels que ALTO, hOCR, et TEI sont fournis par défaut. Les modules de traitement
sont accessibles à la fois par l'API et par la ligne de commande qui permet la
substitution flexible de blocs fonctionnels ou l'utilisation de sous-systèmes
pour compléter ses propres méthodes.

Dans le cas où des défauts raisonnables sont souhaitables mais peuvent être
désavantageux dans les cas marginaux, ils peuvent généralement être désactivés
ou adaptés. Les exemples vont du traitement textuel tel que la prise en charge
de texte bidirectionnel\footnote{L'algorithme Unicode BiDi a des cas où un
balisage explicite de la directionalité peut être requis.} et de la
normalisation du texte au changement des architectures et des paramètres
d'entraînement des réseaux neuronaux artificiels employés dans la segmentation
et la transcription des pages.

Le module de transcription fonctionne comme un classificateur de séquences sans
segmentation, utilisant un réseau neuronal artificiel pour mapper une image
d'une seule ligne de texte en une séquence d'étiquettes qui sont ensuite
mappées en points de code Unicode.  L'ARN utilisé par défaut est un CNN-RNN
hybride entraîné avec la fonction de coût CTC. Un langage simple de
spécification de réseau permet d'adapter le réseau à des tâches spécifiques.
Les précisions des caractères pour un certain nombre de scripts différents
utilisant ce classificateur sont indiquées dans le tableau~\ref{tab:acc}.

La segmentation des pages est assurée par le système de segmentation des
régions et des lignes décrit ci-dessus. Comme d'autres parties du logiciel, il
est hautement configurable et permet la détection de régions et de lignes de
texte arbitraires avec suffisamment de données d'entraînement. Les données
d'entraînement peuvent être fournies dans un certain nombre de formats de
fichiers standard tels que ALTO et PageXML ou via une simple API.

\subsection{L'alignement des Caractères}

Une tâche d'un certain intérêt paléographique est l'alignement automatique de
la transcription du texte avec les glyphes respectifs dans une image. Bien que
cela puisse être fait naïvement avec une approche de segmentation des
caractères similaire aux anciens logiciels de ROC, nous avons évalué une
méthode qui utilise l'alignement implicite de la fonction de coût CTC pour
localiser les graphèmes dans une image, à partir d'une transcription
diplomatique, et nous l'avons comparée à un système SIFT-flow. La méthode est
destinée à fonctionner sur les manuscrits de la mer Morte, des manuscrits très
fragmentaires écrits principalement en hébreu.

Dans un premier stade, les manuscrits hébraïques fragmentaires sont segmentés à
l'aide d'un modèle de segmentation des pages spécifiquement entraîné pour ce
matériel. Les transcriptions diplomatiques par ligne de la base de données QWB
sont ensuite mises en correspondance avec la sortie du segmenteur afin de créer
des données d'entraînement pour un modèle de transcription de manière
semi-automatique.  Environ 2500 lignes provenant de 440 fragments ont ensuite
été utilisées pour faire une apprentissage par transfer d'un nouveau modèle de
transcription à partir d'un modèle de transcription de manuscrit hébraïque
médiéval existant.  Comme les données d'apprentissage varient énormément en
termes de style, les caractères individuels sont souvent gravement dégradés, et
le modèle est entraîné à surajuster sévèrement, le CER est assez élevé avec
environ 30\% sur l'ensemble de validation.

Les activations de ce modèle surajusté sont utilisées pour déterminer les
positions des caractères sur le matériel dans l'ensemble d'entraînage créé
semi-automatiquement. Lorsqu'il est évalué vis-à-vis des positions de glyphes
annotées par l'homme, le système place le caractère le plus proche de la
position réelle 90,3\% du temps avec une IoU moyenne de 0,81, surpassant
significativement la méthode SIFT-flow même lorsqu'il l'ancre avec les
positions brutes des caractères ROC.

\section{eScriptorium}

eScriptorium est une plateforme d'analyse et d'annotation de documents open
source. Elle cherche à combiner des techniques de calcul avec des outils
numériques manuels pour la transcription et l'annotation approfondie de textes
et d'images aux niveaux paléographique, philologique et linguistique. Il
s'adresse aux chercheurs en sciences humaines, mais aussi aux bibliothécaires
et archivistes, aux étudiants, aux informaticiens et au grand public. Issu du
projet Scripta, qui cherche à faciliter l'étude de l'écriture sous toutes ses
formes au fil de l'histoire, ses principes de base sont la transparence, la
flexibilité et l'indépendance de la langue et du système d'écriture.

Ce dernier point est particulièrement important car la gamme des langues et des
systèmes d'écriture étudiés dans le cadre de Scripta est énorme, couvrant le
Proche-Orient ancien, l'Iran et l'Asie centrale, l'Inde, l'Asie du Sud-Est et
de l'Est, ainsi que l'Occident classique et médiéval. Par conséquent, comme
pour le Kraken, un effort concerté a été fait pour réduire les hypothèses sur
le fonctionnement du texte.

eScriptorium utilise le Kraken pour ses besoins en vision par ordinateur.
Ainsi, la construction du pipeline ROC est reflétée dans l'interface
d'eScriptorium, avec une approche par étapes de l'importation des données, de
la segmentation des pages (automatique ou manuelle), de la transcription
(automatique ou manuelle), de l'annotation et de l'exportation.

La nécessité de s'adapter à une grande variété de systèmes d'écriture, en
particulier la volonté de pouvoir traiter des écritures rares et historiques,
impose à eScriptorium certaines restrictions de conception qui vont au-delà des
mesures prises pour rendre les méthodes computationnelles de Kraken
polyvalentes. Par définition, les langages rares manquent de grands ensembles
de données préexistants qui peuvent être utilisés pour lancer le processus de
ROC.  Par conséquent, l'annotation et la vérification manuelles de la
segmentation et de la transcription ne peuvent pas être une simple réflexion
après coup, mais doivent être considérées comme une partie fondamentale de
l'interface, à la fois pour permettre un travail pratique avec les plus petits
ensembles de données qui ne peuvent pas encore être traités avec les méthodes
automatiques mises en œuvre et pour aider au démarrage efficace du traitement
automatique.

Elle empêche également l'utilisation de techniques courantes pour augmenter la
généralisation et la charge de formation des méthodes automatiques telles que
les modèles de langage statistique et les modèles généralisés pour des tâches
comme la segmentation des pages. Les modèles linguistiques puissants pour les
langues à faibles ressources telles que le vietnamien ancien sont tout aussi
irréalistes qu'un segmenteur de pages capable d'extraire avec précision des
lignes d'inscriptions chinoises, de manuscrits arabes, d'incunables et de
journaux avec un seul modèle RNA. Par conséquent, la plate-forme est conçue
pour permettre un apprentissage et un réapprentissage fréquents grâce à des
inventaires de modèles, des interfaces intermédiaires pour l'importation et
l'exportation de données, et des rapports d'évaluation prospectifs à grain fin
comme ceux qui existent déjà dans le Kraken.

Un dernier aspect renforçant ces contraintes de conception dans la plate-forme
provient non pas du matériel source mais du type de travail effectué sur
celui-ci. Les chercheurs en humanités effectuent un large éventail de
recherches en utilisant un grand nombre de paradigmes différents sur le
matériel textuel. Ce pluralisme méthodologique se traduit par des conventions
de transcription différentes, même sur du matériel dans la même langue, en
fonction des préférences particulières du chercheur et de son domaine. Il
existe donc un besoin fondamental de s'adapter aux différentes normes et de les
rendre visibles aux autres, en particulier dans le contexte des systèmes
d'intelligence artificielle qui ne sont, après tout, que de puissants outils
d'inférence statistique. Les systèmes ouverts peuvent aider à communiquer les
normes et les hypothèses de ces procédures, mais il n'en reste pas moins que
pour un simple utilisateur de sciences humaines peu familier avec la
terminologie de l'informatique, celles-ci sont cachées dans une boîte noire
magique (et vice versa).  Les ontologies peuvent principalement combler ce
fossé, mais elles sont complexes à mettre en place et à entretenir et se
heurtent souvent à la nature ad hoc de la recherche. Notre meilleure option
reste de suivre les standards lorsqu'ils existent, d'offrir des interfaces pour
prendre et apporter des données et des artefacts depuis et vers l'eScriptorium,
et d'accepter qu'il est très peu probable qu'un seul outil soit à la fois
pratique et universel.

\section{Conclusions et Perspectives}

En conclusion, nous avons présenté dans cette thèse un travail qui représente
un pas en avant vers la rétro-numérisation pratique des documents en écriture
arabe et des documents historiques et non-latins en général. La segmentation
des pages et la transcription sont maintenant en principe capables de numériser
n'importe quel document en caractères arabes, mais surtout l'inclusion de ces
méthodes dans un système de ROC de bas niveau et un ERV de haut niveau, qui
sont tous deux totalement ouverts à l'adaptation, la réutilisation et le
partage, rendent l'utilisation de ces outils dans les projets de sciences
humaines numériques, petits et grands, beaucoup plus attrayante.

Il est clair qu'un travail substantiel reste à faire. Nous avons étudié les
exigences générales d'un système de ROC à usage général en écriture arabe et
validé l'état de l'art au début de la thèse dans deux études. Bien que les
questions les plus urgentes, l'analyse de la mise en page, des méthodes de
transcription plus puissantes et de meilleurs outils pour la création, la
conservation et la diffusion des données, aient été résolues dans une large
mesure, toutes les tâches ne sont pas actuellement résolues de manière
satisfaisante.

La tâche la plus urgente pour la ROC des manuscrits arabes est la détermination
de l'ordre de lecture. Comme décrit ci-dessus, la recherche sur ce sujet est
rare et les méthodes existantes sont des heuristiques artisanales incapables de
traiter la structure souvent complexe des manuscrits historiques. Alors que les
ensembles de données sont inexistants ou implicitement cachés dans des
ensembles de données pour d'autres tâches, les capacités de l'intelligence
artificielle pour le raisonnement spatial avec un grand nombre d'objets ont
augmenté ces dernières années avec le développement des réseaux neuronaux de
graphes.

Bien que les systèmes de ROC de pointe sont capables de réaliser des exploits
impressionnants même sur des documents très dégradés et atypiques, avec des
exigences en matière de données d'entraînement plus ergonomiques que jamais, le
repérage des données d'entraînement est toujours la tâche la plus longue des
techniques modernes de vision par ordinateur. Alors que nous pouvons maintenant
utiliser efficacement l'apprentissage par transfert pour adapter les modèles
existants à de nouveaux documents avec des quantités minimales de données, des
méthodes d'adaptation de domaine plus avancées offrent de grandes promesses
pour rendre plus de documents accessibles sans intervention humaine.

Le développement d'eScriptorium va sûrement se poursuivre et intégrer les
progrès des méthodes automatiques dans la mesure où il aide la recherche en
sciences humaines. Les pistes non explorées dans cette thèse comprennent les
opérations non textuelles, telles que diverses tâches de classification
d'images, la datation, le regroupement de documents similaires ou la détection
de la réutilisation de textes.

Enfin, même avec la disponibilité d'outils de vision par ordinateur puissants
et ouverts, le paysage des ensembles de données reste fracturé. Alors que les
chercheurs reconnaissent plus que jamais l'importance du partage des données
pour faire progresser non seulement les sciences humaines mais aussi la
recherche informatique, le moyen préféré pour y parvenir reste le dépôt github
profane avec un fichier README non descriptif. Une combinaison d'ERV conscients
de l'importance de métadonnées appropriées, d'un apport élargi de la pratique
archivistique dans la recherche scientifique, et de l'utilisation
d'infrastructures de données de recherche ouvertes comme c'est déjà le cas dans
d'autres disciplines scientifiques, a le potentiel d'améliorer considérablement
cet état de fait dans les prochaines années.

\end{french}

